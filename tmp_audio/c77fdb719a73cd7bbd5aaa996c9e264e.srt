1
00:00:00,300 --> 00:01:14,279
Speaker 0: I'm excited to introduce a new course, Large Language Models with Semantic Search, built in partnership with Cohere. In this course, you'll learn how to incorporate large language models or LLMs into information search in your own applications. Let's say you run a website with a lot of articles, picture Wikipedia or a news media site, or an online shopping site with a lot of products. Many such websites have had a keyword search capability for a long time to let users find relevant information or products. But LLMs are making search on such sites much better. You can now let users ask questions and have your site search for the relevant information and then use that information to generate a text response. LLMs are used in two ways in this process. First, to improve the relevance of the retrieved information via a technique called text embeddings. And second, to generate the response. Search with LLMs is an important and useful capability that you learn how to build yourself in this course. I'm delighted that this course is taught by Jay Alomar and Luis Serrano, both of whom are experienced machine learning engineers, together with their colleagues at Cohere.

2
00:01:14,820 --> 00:01:39,799
Speaker 2: The two main ideas we'll discuss for language model powered search are dense retrieval and re-ranking. Dense retrieval relies on the idea of embeddings. These are numeric vectors that capture the meaning of a text. It then uses them to provide search results that are generally better than keyword search. Re-ranking on the other hand is probably the fastest way to inject the intelligence of a large language model into a search system.

3
00:01:40,723 --> 00:02:16,199
Speaker 1: Yeah, not only does that improve the search systems that you built, but that also leads to creating the next generation of LLMs that are powered by search. LLMs are great at many things, but they're not especially trained to retrieve factual information. To improve them in that regard, we can provide them with information that we retrieve from a search step. This makes them more factual and gives us a better ability to provide the model with up-to-date information without the need to retrain these massive models. This is precisely what a retrieval augmented LLM is. And creating this and using steps to optimize it is what we'll be teaching you in this course.

4
00:02:16,880 --> 00:02:43,299
Speaker 0: So after finishing this course, you'll be able to incorporate search with LLMs into your own applications. And I believe you also come away with a deeper understanding of how some key technical foundations of LLMs, such as embeddings work, which will help you to become a better builder with LLMs. For example, maybe inspiring you to think of creative ways you can use LLMs in even other applications. I hope you enjoy the course.

